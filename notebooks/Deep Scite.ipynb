{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Deep Scite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the model, for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep-scite-model-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from deepscite import model\n",
    "from deepscite import utils\n",
    "from deepscite import train\n",
    "import ruamel.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"../\"\n",
    "data_dir = os.path.join(base_dir, \"data/noon/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the parameters we want to use during training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the `conf` global state that we use in various places in the model.\n",
    "conf = tf.app.flags.FLAGS\n",
    "\n",
    "conf.embedded_word_size  = 150\n",
    "conf.word_vector_size    = 200\n",
    "conf.conv_size           = 3\n",
    "conf.conv_stride         = 1\n",
    "conf.conv_features       = 1\n",
    "conf.iterations          = 300\n",
    "conf.learning_rate       = 1e-3\n",
    "conf.weights_reg_scale   = 1e-6\n",
    "conf.activity_reg_scale  = 1e-6\n",
    "conf.embedding_reg_scale = 1e-6\n",
    "conf.save_path           = os.path.join(base_dir, \"./checkpoints/noon\")\n",
    "conf.log_path            = \"/tmp/tf-checkpoints/deepscite-noon\"\n",
    "conf.data_dir            = data_dir\n",
    "\n",
    "checkpoint_path    = os.path.join(base_dir, \"checkpoints/noon/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising new model...\n",
      "Iteration #0, Loss: 0.6931463479995728, α: 0.5.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-0.\n",
      "Iteration #0, Validation-set accuracy: 0.49799999594688416.\n",
      "Iteration #10, Loss: 1.023919701576233, α: 0.5001515746116638.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-10.\n",
      "Iteration #10, Validation-set accuracy: 0.8220000267028809.\n",
      "Iteration #20, Loss: 0.7489159107208252, α: 0.5026029944419861.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-20.\n",
      "Iteration #20, Validation-set accuracy: 0.8389999866485596.\n",
      "Iteration #30, Loss: 0.6852636337280273, α: 0.5056034922599792.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-30.\n",
      "Iteration #30, Validation-set accuracy: 0.8429999947547913.\n",
      "Iteration #40, Loss: 0.6179088950157166, α: 0.5090567469596863.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-40.\n",
      "Iteration #40, Validation-set accuracy: 0.8479999899864197.\n",
      "Iteration #50, Loss: 0.566624104976654, α: 0.5127959847450256.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-50.\n",
      "Iteration #50, Validation-set accuracy: 0.8410000205039978.\n",
      "Iteration #60, Loss: 0.5123057961463928, α: 0.5166715979576111.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-60.\n",
      "Iteration #60, Validation-set accuracy: 0.8420000076293945.\n",
      "Iteration #70, Loss: 0.47073498368263245, α: 0.5203702449798584.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-70.\n",
      "Iteration #70, Validation-set accuracy: 0.8510000109672546.\n",
      "Iteration #80, Loss: 0.44123178720474243, α: 0.5238275527954102.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-80.\n",
      "Iteration #80, Validation-set accuracy: 0.8550000190734863.\n",
      "Iteration #90, Loss: 0.4078579545021057, α: 0.5270022749900818.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-90.\n",
      "Iteration #90, Validation-set accuracy: 0.8560000061988831.\n",
      "Iteration #100, Loss: 0.3750966191291809, α: 0.5299534797668457.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-100.\n",
      "Iteration #100, Validation-set accuracy: 0.8529999852180481.\n",
      "Iteration #110, Loss: 0.39299333095550537, α: 0.532818615436554.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-110.\n",
      "Iteration #110, Validation-set accuracy: 0.8550000190734863.\n",
      "Iteration #120, Loss: 0.3629750609397888, α: 0.5353860259056091.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-120.\n",
      "Iteration #120, Validation-set accuracy: 0.8560000061988831.\n",
      "Iteration #130, Loss: 0.36914700269699097, α: 0.5378422737121582.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-130.\n",
      "Iteration #130, Validation-set accuracy: 0.8600000143051147.\n",
      "Iteration #140, Loss: 0.3335404396057129, α: 0.5402000546455383.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-140.\n",
      "Iteration #140, Validation-set accuracy: 0.8640000224113464.\n",
      "Iteration #150, Loss: 0.3154831528663635, α: 0.5425455570220947.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-150.\n",
      "Iteration #150, Validation-set accuracy: 0.8650000095367432.\n",
      "Iteration #160, Loss: 0.3176337480545044, α: 0.5450243949890137.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-160.\n",
      "Iteration #160, Validation-set accuracy: 0.8619999885559082.\n",
      "Iteration #170, Loss: 0.28871431946754456, α: 0.5475322604179382.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-170.\n",
      "Iteration #170, Validation-set accuracy: 0.8629999756813049.\n",
      "Iteration #180, Loss: 0.2682073712348938, α: 0.5499554872512817.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-180.\n",
      "Iteration #180, Validation-set accuracy: 0.859000027179718.\n",
      "Iteration #190, Loss: 0.2709682583808899, α: 0.5523027181625366.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-190.\n",
      "Iteration #190, Validation-set accuracy: 0.8629999756813049.\n",
      "Iteration #200, Loss: 0.2637026309967041, α: 0.5547624826431274.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-200.\n",
      "Iteration #200, Validation-set accuracy: 0.8659999966621399.\n",
      "Iteration #210, Loss: 0.2695176601409912, α: 0.5571551322937012.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-210.\n",
      "Iteration #210, Validation-set accuracy: 0.8610000014305115.\n",
      "Iteration #220, Loss: 0.27654707431793213, α: 0.5594083070755005.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-220.\n",
      "Iteration #220, Validation-set accuracy: 0.8619999885559082.\n",
      "Iteration #230, Loss: 0.2197912037372589, α: 0.5616894364356995.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-230.\n",
      "Iteration #230, Validation-set accuracy: 0.8560000061988831.\n",
      "Iteration #240, Loss: 0.24630442261695862, α: 0.5639524459838867.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-240.\n",
      "Iteration #240, Validation-set accuracy: 0.8519999980926514.\n",
      "Iteration #250, Loss: 0.23931559920310974, α: 0.5662196278572083.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-250.\n",
      "Iteration #250, Validation-set accuracy: 0.8600000143051147.\n",
      "Iteration #260, Loss: 0.2314770370721817, α: 0.5684893727302551.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-260.\n",
      "Iteration #260, Validation-set accuracy: 0.8529999852180481.\n",
      "Iteration #270, Loss: 0.2251550853252411, α: 0.5707225799560547.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-270.\n",
      "Iteration #270, Validation-set accuracy: 0.8569999933242798.\n",
      "Iteration #280, Loss: 0.21485580503940582, α: 0.572901725769043.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-280.\n",
      "Iteration #280, Validation-set accuracy: 0.8539999723434448.\n",
      "Iteration #290, Loss: 0.22472772002220154, α: 0.5751131176948547.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-290.\n",
      "Iteration #290, Validation-set accuracy: 0.8539999723434448.\n",
      "Iteration #300, Loss: 0.21965447068214417, α: 0.5771969556808472.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/checkpoint-300.\n",
      "Iteration #300, Validation-set accuracy: 0.8510000109672546.\n"
     ]
    }
   ],
   "source": [
    "conf.minibatch_size = 1000\n",
    "tf.reset_default_graph()\n",
    "train.main(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed in a single paper (title, abstract) into DeepScite and see what it thinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference step\n",
    "\n",
    "Re-arrange or enter your own titles to see what DeepScite, training on Noons data, will think of it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "arxiv_id = \"1609.05011\"\n",
    "title = \"Convex separation from convex optimization for large-scale problems\"\n",
    "abstract = r\"\"\"\n",
    "    We present a scheme, based on Gilbert's algorithm for quadratic minimization\n",
    "    [SIAM J. Contrl., vol. 4, pp. 61-80, 1966], to prove separation between a\n",
    "    point and an arbitrary convex set S⊂ℝnS⊂Rn via calls to an oracle able to\n",
    "    perform linear optimizations over SS. Compared to other methods, our scheme\n",
    "    has almost negligible memory requirements and the number of calls to the\n",
    "    optimization oracle does not depend on the dimensionality nn of the underlying\n",
    "    space. We study the speed of convergence of the scheme under different\n",
    "    promises on the shape of the set SS and/or the location of the point,\n",
    "    validating the accuracy of our theoretical bounds with numerical\n",
    "    examples. Finally, we present some applications of the scheme in\n",
    "    quantum information theory. There we find that our algorithm\n",
    "    out-performs existing linear programming methods for certain large\n",
    "    scale problems, allowing us to certify nonlocality in bipartite\n",
    "    scenarios with upto 4242 measurement settings. We apply the algorithm\n",
    "    to upper bound the visibility of two-qubit Werner states, hence\n",
    "    improving known lower bounds on Grothendieck's constant KG(3)KG(3).\n",
    "    Similarly, we compute new upper bounds on the visibility of GHZ\n",
    "    states and on the steerability limit of Werner states for a fixed\n",
    "    number of measurement settings.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Bad one\n",
    "title = \"Quantum integration technique for closed quantum groups\"\n",
    "abstract = \"Here we present a new integration technique in functional analysis that ...\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Very good\n",
    "title = \"Quantum algorithm for computing the determinant\"\n",
    "abstract = r\"\"\"\n",
    "    We present a quantum algorithm to compute the determinant in Polynomial\n",
    "    time. It has long been known ...\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the text into the format needed by the model. Each word is mapped to the index of the vector in the word embedding matrix (i.e. it's index in the `vocab.txt` file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep-scite-model-with-vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = utils.load_vocabulary(data_dir)\n",
    "vocab_dict = {}\n",
    "for k, w in enumerate(vocab_list):\n",
    "    vocab_dict[w] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordids_for(s):\n",
    "    r = [vocab_dict[w] for w in utils.to_words(s) if w in vocab_dict ]\n",
    "    if r == []:\n",
    "        raise Exception(\"Found no words at all!\")\n",
    "    return \" \".join(map(str, r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputs = [ {\"id\": arxiv_id, \n",
    "            \"wordset_1_ids\": get_wordids_for(title), \n",
    "            \"wordset_2_ids\": get_wordids_for(abstract) } ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the model and emit a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m = model.JointEmbeddingModelForBinaryClassification(conf.embedded_word_size)\n",
    "\n",
    "# TensorFlow is uses a lot of global state. As a result, if we \n",
    "# wish to re-run this cell many times, we need to have this\n",
    "# statement here to ensure nothing is kept over.\n",
    "tf.reset_default_graph()\n",
    "\n",
    "conf.minibatch_size      = 1 # We're only inputting one piece of data - a single paper.\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    model_params = m.graph(\n",
    "        conf.minibatch_size,\n",
    "        len(vocab_list),\n",
    "        conf.word_vector_size,\n",
    "        conf.conv_size,\n",
    "        conf.conv_stride,\n",
    "        conf.conv_features\n",
    "    )\n",
    "    \n",
    "    # Load the trained weights\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint = tf.train.latest_checkpoint(checkpoint_path)\n",
    "    \n",
    "    if not checkpoint:\n",
    "        raise Exception(\"Couldn't find checkpoint at: {}\".format(checkpoint_path))\n",
    "    \n",
    "    saver.restore(sess, checkpoint)\n",
    "    \n",
    "    X1, X2, _, M1, M2, S1, S2, subset = train.get_datapoints(inputs)\n",
    "    data = {model_params.wordset_1: X1,\n",
    "            model_params.wordset_2: X2,\n",
    "            model_params.wordset_1_masks: M1,\n",
    "            model_params.wordset_2_masks: M2,\n",
    "            model_params.wordset_1_lengths: S1,\n",
    "            model_params.wordset_2_lengths: S2}\n",
    "    \n",
    "\n",
    "    # Calculate the recommendations\n",
    "    set1_activations, set2_activations, final_probs, alpha = sess.run([\n",
    "        tf.squeeze(model_params.conv_wordset_1_activity, [2,3]),\n",
    "        tf.squeeze(model_params.conv_wordset_2_activity, [2,3]),\n",
    "        model_params.final_probs,\n",
    "        model_params.alpha], \n",
    "        feed_dict=data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With what probability would Noon *scite* this paper?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.95597804"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_probs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_words    = utils.to_words(title)\n",
    "abstract_words = utils.to_words(abstract)\n",
    "\n",
    "threshold = 5\n",
    "\n",
    "good_title_words = [ title_words[k] \n",
    "                    for k, v in enumerate(set1_activations[0]) if v > threshold]\n",
    "bad_title_words  = [ title_words[k] \n",
    "                    for k, v in enumerate(set1_activations[0]) if v < -threshold]\n",
    "\n",
    "good_abstract_words = [ abstract_words[k] \n",
    "                       for k, v in enumerate(set2_activations[0]) if v > threshold]\n",
    "bad_abstract_words  = [ abstract_words[k] \n",
    "                       for k, v in enumerate(set2_activations[0]) if v < -threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantum', 'algorithm']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_title_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['quantum', 'algorithm']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "good_abstract_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad_abstract_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting parameter\n",
    "\n",
    "$$\n",
    "    p = \\alpha * \\text{titles} + (1-\\alpha) * \\text{abstracts}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57741034"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
