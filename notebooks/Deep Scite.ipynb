{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimenting with Deep Scite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the model, for reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep-scite-model-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from deepscite import model\n",
    "from deepscite import utils\n",
    "from deepscite import train\n",
    "import ruamel.yaml\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "base_dir = \"../\"\n",
    "data_dir = os.path.join(base_dir, \"data/noon/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the parameters we want to use during training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Update the `conf` global state that we use in various places in the model.\n",
    "conf = tf.app.flags.FLAGS\n",
    "\n",
    "conf.embedded_word_size  = 250\n",
    "conf.word_vector_size    = 500\n",
    "conf.conv_size           = 3\n",
    "conf.conv_stride         = 1\n",
    "conf.conv_features       = 1\n",
    "conf.iterations          = 100\n",
    "conf.learning_rate       = 1e-3\n",
    "conf.weights_reg_scale   = 1e-6\n",
    "conf.activity_reg_scale  = 1e-6\n",
    "conf.embedding_reg_scale = 1e-6\n",
    "conf.save_path           = os.path.join(base_dir, \"./checkpoints/noon\")\n",
    "conf.log_path            = \"/tmp/tf-checkpoints/deepscite-noon\"\n",
    "conf.data_dir            = data_dir\n",
    "\n",
    "checkpoint_path    = os.path.join(base_dir, \"checkpoints/noon/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialising new model...\n",
      "WARNING:tensorflow:From /home/noon/dev/deep-scite/deepscite/train.py:181 in train.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Iteration #0, Loss: 0.6931468844413757, α: 0.5.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-0.\n",
      "Iteration #0, Validation-set accuracy: 0.5019999742507935.\n",
      "Iteration #10, Loss: 2.400573253631592, α: 0.49991798400878906.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-10.\n",
      "Iteration #10, Validation-set accuracy: 0.7900000214576721.\n",
      "Iteration #20, Loss: 1.081951379776001, α: 0.5023369193077087.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-20.\n",
      "Iteration #20, Validation-set accuracy: 0.8180000185966492.\n",
      "Iteration #30, Loss: 0.8039199709892273, α: 0.5053424835205078.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-30.\n",
      "Iteration #30, Validation-set accuracy: 0.8100000023841858.\n",
      "Iteration #40, Loss: 0.6196919083595276, α: 0.5087844133377075.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-40.\n",
      "Iteration #40, Validation-set accuracy: 0.8059999942779541.\n",
      "Iteration #50, Loss: 0.5336254239082336, α: 0.512456476688385.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-50.\n",
      "Iteration #50, Validation-set accuracy: 0.8259999752044678.\n",
      "Iteration #60, Loss: 0.467700332403183, α: 0.5159721374511719.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-60.\n",
      "Iteration #60, Validation-set accuracy: 0.828000009059906.\n",
      "Iteration #70, Loss: 0.4269375503063202, α: 0.5192067623138428.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-70.\n",
      "Iteration #70, Validation-set accuracy: 0.8320000171661377.\n",
      "Iteration #80, Loss: 0.4095444977283478, α: 0.522005558013916.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-80.\n",
      "Iteration #80, Validation-set accuracy: 0.8600000143051147.\n",
      "Iteration #90, Loss: 0.37680763006210327, α: 0.5243531465530396.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-90.\n",
      "Iteration #90, Validation-set accuracy: 0.8500000238418579.\n",
      "Iteration #100, Loss: 0.3698449730873108, α: 0.5261351466178894.\n",
      "Checkpointed: /tmp/tf-checkpoints/deepscite-noon/def/checkpoint-100.\n",
      "Iteration #100, Validation-set accuracy: 0.8640000224113464.\n"
     ]
    }
   ],
   "source": [
    "conf.minibatch_size = 500\n",
    "tf.reset_default_graph()\n",
    "train.main(_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's feed in a single paper (title, abstract) into DeepScite and see what it thinks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the text into the format needed by the model. Each word is mapped to the index of the vector in the word embedding matrix (i.e. it's index in the `vocab.txt` file.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](deep-scite-model-with-vectors.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab_list = utils.load_vocabulary(data_dir)\n",
    "vocab_dict = {}\n",
    "for k, w in enumerate(vocab_list):\n",
    "    vocab_dict[w] = k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_wordids_for(s):\n",
    "    r = [vocab_dict[w] for w in utils.to_words(s) if w in vocab_dict ]\n",
    "    if r == []:\n",
    "        raise Exception(\"Found no words at all!\")\n",
    "    return \" \".join(map(str, r))\n",
    "\n",
    "def words_to_html(words, activations, threshold=5):\n",
    "    good_words = []\n",
    "    bad_words  = []\n",
    "\n",
    "    elts = []\n",
    "\n",
    "    for k, w in enumerate(words):\n",
    "        activation = round(float(activations[k]), 2)\n",
    "\n",
    "        style = \"\"\n",
    "        if activation > threshold:\n",
    "            good_words.append(w)\n",
    "            style = \"color: blue !important;\"\n",
    "\n",
    "        if activation < -threshold:\n",
    "            bad_words.append(w)\n",
    "            style = \"color: red !important;\"\n",
    "\n",
    "        elts.append(\"<span style='{}' title='({},{})'>{}</span>\".format(\n",
    "                style,\n",
    "                activation,\n",
    "                round(float(activations[k]), 2), w))\n",
    "    \n",
    "    return \" \".join(elts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Load the model and emit a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def infer(title, abstract):\n",
    "    inputs = [ {\"id\": arxiv_id, \n",
    "                \"wordset_1_ids\": get_wordids_for(title), \n",
    "                \"wordset_2_ids\": get_wordids_for(abstract) } ]\n",
    "\n",
    "    m = model.JointEmbeddingModelForBinaryClassification(conf.embedded_word_size)\n",
    "\n",
    "    # TensorFlow is uses a lot of global state. As a result, if we \n",
    "    # wish to re-run this cell many times, we need to have this\n",
    "    # statement here to ensure nothing is kept over.\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # We're only inputting one piece of data - a single paper.\n",
    "    conf.minibatch_size = 1\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "\n",
    "        model_params = m.graph(\n",
    "            conf.minibatch_size,\n",
    "            len(vocab_list),\n",
    "            conf.word_vector_size,\n",
    "            conf.conv_size,\n",
    "            conf.conv_stride,\n",
    "            conf.conv_features\n",
    "        )\n",
    "\n",
    "        # Load the trained weights\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint = tf.train.latest_checkpoint(checkpoint_path)\n",
    "\n",
    "        if not checkpoint:\n",
    "            raise Exception(\"Couldn't find checkpoint at: {}\".format(checkpoint_path))\n",
    "\n",
    "        saver.restore(sess, checkpoint)\n",
    "\n",
    "        X1, X2, _, M1, M2, S1, S2, subset = train.get_datapoints(inputs)\n",
    "        data = {model_params.wordset_1: X1,\n",
    "                model_params.wordset_2: X2,\n",
    "                model_params.wordset_1_masks: M1,\n",
    "                model_params.wordset_2_masks: M2,\n",
    "                model_params.wordset_1_lengths: S1,\n",
    "                model_params.wordset_2_lengths: S2}\n",
    "\n",
    "\n",
    "        # Calculate the recommendations\n",
    "        set1_activations, set2_activations, final_probs, alpha = sess.run([\n",
    "            tf.squeeze(model_params.conv_wordset_1_activity, [2,3]),\n",
    "            tf.squeeze(model_params.conv_wordset_2_activity, [2,3]),\n",
    "            model_params.final_probs,\n",
    "            model_params.alpha], \n",
    "            feed_dict=data)\n",
    "    \n",
    "    return set1_activations[0], set2_activations[0], final_probs[0], alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With what probability would Noon *scite* this paper?\n",
    "\n",
    "Enter candidate tiles and abstracts below. You can find inspiration over at [SciRate](https://scirate.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Very good\n",
    "title = r\"\"\"\n",
    "Universal Quantum Hamiltonians\n",
    "\"\"\"\n",
    "\n",
    "abstract = r\"\"\"\n",
    "Quantum many-body systems exhibit a bewilderingly diverse range of behaviours. \n",
    "Here, we prove that all the physics of every other quantum many-body system is\n",
    "replicated in certain simple, \"universal\" quantum spin-lattice models. We first \n",
    "characterise precisely and in full generality what it means for one quantum \n",
    "many-body system to replicate the entire physics of another. We then fully \n",
    "classify two-qubit interactions, determining which are universal in this very \n",
    "strong sense and showing that certain simple spin-lattice models are already \n",
    "universal. Examples include the Heisenberg and XY models on a 2D square lattice\n",
    "(with non-uniform coupling strengths). This shows that locality, symmetry, and \n",
    "spatial dimension need not constrain the physics of quantum many-body systems. \n",
    "Our results put the practical field of analogue Hamiltonian simulation on a \n",
    "rigorous footing and show that far simpler systems than previously thought may \n",
    "be viable simulators. We also take a first step towards justifying why error \n",
    "correction may not be required for this application of quantum information \n",
    "technology.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scite Probability: 99.82%\n"
     ]
    }
   ],
   "source": [
    "set1_activations, set2_activations, final_probs, alpha = infer(title, abstract)\n",
    "print(\"Scite Probability: {0:2.2f}%\".format(final_probs*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style='color: blue !important;' title='(8.95,8.95)'>universal</span> <span style='color: blue !important;' title='(13.06,13.06)'>quantum</span> <span style='color: blue !important;' title='(10.37,10.37)'>hamiltonians</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style='color: blue !important;' title='(8.98,8.98)'>quantum</span> <span style='color: blue !important;' title='(9.21,9.21)'>many-body</span> <span style='' title='(1.26,1.26)'>systems</span> <span style='' title='(-0.75,-0.75)'>exhibit</span> <span style='' title='(0.3,0.3)'>a</span> <span style='' title='(-0.34,-0.34)'>bewilderingly</span> <span style='' title='(-1.42,-1.42)'>diverse</span> <span style='' title='(-2.24,-2.24)'>range</span> <span style='' title='(-0.83,-0.83)'>of</span> <span style='' title='(2.26,2.26)'>behaviours</span> <span style='' title='(2.1,2.1)'>.</span> <span style='' title='(2.58,2.58)'>here</span> <span style='' title='(-0.25,-0.25)'>,</span> <span style='' title='(0.14,0.14)'>we</span> <span style='' title='(1.33,1.33)'>prove</span> <span style='' title='(1.42,1.42)'>that</span> <span style='' title='(1.02,1.02)'>all</span> <span style='' title='(-1.44,-1.44)'>the</span> <span style='' title='(-0.34,-0.34)'>physics</span> <span style='' title='(-0.06,-0.06)'>of</span> <span style='color: blue !important;' title='(7.87,7.87)'>every</span> <span style='color: blue !important;' title='(9.66,9.66)'>other</span> <span style='color: blue !important;' title='(8.14,8.14)'>quantum</span> <span style='' title='(0.58,0.58)'>many-body</span> <span style='' title='(-1.34,-1.34)'>system</span> <span style='' title='(-2.2,-2.2)'>is</span> <span style='' title='(-1.94,-1.94)'>replicated</span> <span style='' title='(-1.93,-1.93)'>in</span> <span style='' title='(0.26,0.26)'>certain</span> <span style='' title='(1.86,1.86)'>simple</span> <span style='' title='(5.12,5.12)'>,</span> <span style='color: blue !important;' title='(6.4,6.4)'>``</span> <span style='color: blue !important;' title='(11.67,11.67)'>universal</span> <span style='color: blue !important;' title='(8.23,8.23)'>''</span> <span style='color: blue !important;' title='(7.49,7.49)'>quantum</span> <span style='' title='(0.13,0.13)'>spin-lattice</span> <span style='' title='(0.7,0.7)'>models</span> <span style='' title='(-0.38,-0.38)'>.</span> <span style='' title='(-0.13,-0.13)'>we</span> <span style='' title='(-0.08,-0.08)'>first</span> <span style='' title='(-0.4,-0.4)'>characterise</span> <span style='' title='(-2.25,-2.25)'>precisely</span> <span style='' title='(-2.64,-2.64)'>and</span> <span style='' title='(-1.79,-1.79)'>in</span> <span style='' title='(2.48,2.48)'>full</span> <span style='' title='(3.12,3.12)'>generality</span> <span style='' title='(1.87,1.87)'>what</span> <span style='' title='(-0.16,-0.16)'>it</span> <span style='' title='(-0.15,-0.15)'>means</span> <span style='color: blue !important;' title='(7.82,7.82)'>for</span> <span style='color: blue !important;' title='(9.49,9.49)'>one</span> <span style='color: blue !important;' title='(8.14,8.14)'>quantum</span> <span style='' title='(0.88,0.88)'>many-body</span> <span style='' title='(-0.52,-0.52)'>system</span> <span style='' title='(-0.26,-0.26)'>to</span> <span style='' title='(0.25,0.25)'>replicate</span> <span style='' title='(0.07,0.07)'>the</span> <span style='' title='(-0.18,-0.18)'>entire</span> <span style='' title='(-0.96,-0.96)'>physics</span> <span style='' title='(-1.61,-1.61)'>of</span> <span style='' title='(-0.34,-0.34)'>another</span> <span style='' title='(0.38,0.38)'>.</span> <span style='' title='(2.28,2.28)'>we</span> <span style='' title='(2.23,2.23)'>then</span> <span style='' title='(3.59,3.59)'>fully</span> <span style='' title='(1.46,1.46)'>classify</span> <span style='' title='(1.62,1.62)'>two-qubit</span> <span style='' title='(-0.95,-0.95)'>interactions</span> <span style='' title='(-0.59,-0.59)'>,</span> <span style='' title='(-1.29,-1.29)'>determining</span> <span style='' title='(2.63,2.63)'>which</span> <span style='' title='(1.13,1.13)'>are</span> <span style='' title='(1.88,1.88)'>universal</span> <span style='' title='(-1.7,-1.7)'>in</span> <span style='' title='(-0.49,-0.49)'>this</span> <span style='' title='(-0.15,-0.15)'>very</span> <span style='' title='(-0.54,-0.54)'>strong</span> <span style='' title='(1.1,1.1)'>sense</span> <span style='' title='(1.01,1.01)'>and</span> <span style='' title='(1.66,1.66)'>showing</span> <span style='' title='(0.73,0.73)'>that</span> <span style='' title='(-0.37,-0.37)'>certain</span> <span style='' title='(0.41,0.41)'>simple</span> <span style='' title='(-0.02,-0.02)'>spin-lattice</span> <span style='' title='(0.8,0.8)'>models</span> <span style='' title='(3.28,3.28)'>are</span> <span style='' title='(3.61,3.61)'>already</span> <span style='' title='(3.71,3.71)'>universal</span> <span style='' title='(1.02,1.02)'>.</span> <span style='' title='(0.58,0.58)'>examples</span> <span style='' title='(-0.3,-0.3)'>include</span> <span style='' title='(-1.66,-1.66)'>the</span> <span style='' title='(-1.3,-1.3)'>heisenberg</span> <span style='' title='(-0.34,-0.34)'>and</span> <span style='' title='(-0.31,-0.31)'>xy</span> <span style='' title='(0.03,0.03)'>models</span> <span style='' title='(-1.08,-1.08)'>on</span> <span style='' title='(-0.45,-0.45)'>a</span> <span style='' title='(-0.46,-0.46)'>2d</span> <span style='' title='(-0.83,-0.83)'>square</span> <span style='' title='(-1.47,-1.47)'>lattice</span> <span style='' title='(-1.54,-1.54)'>(</span> <span style='' title='(-4.38,-4.38)'>with</span> <span style='' title='(-4.38,-4.38)'>non-uniform</span> <span style='' title='(-4.97,-4.97)'>coupling</span> <span style='' title='(-1.52,-1.52)'>strengths</span> <span style='' title='(-0.56,-0.56)'>)</span> <span style='' title='(-0.69,-0.69)'>.</span> <span style='' title='(0.24,0.24)'>this</span> <span style='' title='(1.03,1.03)'>shows</span> <span style='' title='(2.24,2.24)'>that</span> <span style='' title='(1.18,1.18)'>locality</span> <span style='' title='(0.19,0.19)'>,</span> <span style='' title='(-0.72,-0.72)'>symmetry</span> <span style='' title='(-1.16,-1.16)'>,</span> <span style='' title='(-1.59,-1.59)'>and</span> <span style='' title='(1.05,1.05)'>spatial</span> <span style='' title='(3.7,3.7)'>dimension</span> <span style='' title='(3.22,3.22)'>need</span> <span style='' title='(0.38,0.38)'>not</span> <span style='' title='(-1.14,-1.14)'>constrain</span> <span style='' title='(-1.44,-1.44)'>the</span> <span style='color: blue !important;' title='(6.1,6.1)'>physics</span> <span style='color: blue !important;' title='(7.89,7.89)'>of</span> <span style='color: blue !important;' title='(9.21,9.21)'>quantum</span> <span style='' title='(1.83,1.83)'>many-body</span> <span style='' title='(1.21,1.21)'>systems</span> <span style='' title='(0.72,0.72)'>.</span> <span style='' title='(1.05,1.05)'>our</span> <span style='' title='(-1.09,-1.09)'>results</span> <span style='' title='(0.75,0.75)'>put</span> <span style='' title='(-1.5,-1.5)'>the</span> <span style='' title='(-1.87,-1.87)'>practical</span> <span style='' title='(-3.45,-3.45)'>field</span> <span style='' title='(0.19,0.19)'>of</span> <span style='' title='(3.56,3.56)'>analogue</span> <span style='' title='(2.99,2.99)'>hamiltonian</span> <span style='' title='(1.58,1.58)'>simulation</span> <span style='' title='(-1.03,-1.03)'>on</span> <span style='' title='(-0.66,-0.66)'>a</span> <span style='' title='(-1.28,-1.28)'>rigorous</span> <span style='' title='(0.08,0.08)'>footing</span> <span style='' title='(1.18,1.18)'>and</span> <span style='' title='(1.87,1.87)'>show</span> <span style='' title='(2.0,2.0)'>that</span> <span style='' title='(1.47,1.47)'>far</span> <span style='' title='(2.04,2.04)'>simpler</span> <span style='' title='(0.55,0.55)'>systems</span> <span style='' title='(1.47,1.47)'>than</span> <span style='' title='(0.78,0.78)'>previously</span> <span style='' title='(1.4,1.4)'>thought</span> <span style='' title='(0.25,0.25)'>may</span> <span style='' title='(1.4,1.4)'>be</span> <span style='' title='(0.72,0.72)'>viable</span> <span style='' title='(0.93,0.93)'>simulators</span> <span style='' title='(-0.19,-0.19)'>.</span> <span style='' title='(1.38,1.38)'>we</span> <span style='' title='(1.21,1.21)'>also</span> <span style='' title='(1.02,1.02)'>take</span> <span style='' title='(0.65,0.65)'>a</span> <span style='' title='(2.98,2.98)'>first</span> <span style='' title='(4.4,4.4)'>step</span> <span style='' title='(4.91,4.91)'>towards</span> <span style='color: blue !important;' title='(5.64,5.64)'>justifying</span> <span style='color: blue !important;' title='(6.16,6.16)'>why</span> <span style='' title='(4.45,4.45)'>error</span> <span style='' title='(3.05,3.05)'>correction</span> <span style='' title='(2.1,2.1)'>may</span> <span style='' title='(3.16,3.16)'>not</span> <span style='' title='(1.84,1.84)'>be</span> <span style='' title='(1.8,1.8)'>required</span> <span style='' title='(0.16,0.16)'>for</span> <span style='' title='(-1.42,-1.42)'>this</span> <span style='' title='(4.91,4.91)'>application</span> <span style='color: blue !important;' title='(10.18,10.18)'>of</span> <span style='color: blue !important;' title='(12.22,12.22)'>quantum</span> <span style='' title='(4.86,4.86)'>information</span> <span style='' title='(0.52,0.52)'>technology</span> <span style='' title='(-0.0,-0.0)'>.</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "threshold = np.mean([x for x in set1_activations if x > 0]) / 2\n",
    "\n",
    "title_words    = utils.to_words(title)\n",
    "abstract_words = utils.to_words(abstract)\n",
    "\n",
    "display(HTML(words_to_html(title_words,    set1_activations, threshold)))\n",
    "display(HTML(words_to_html(abstract_words, set2_activations, threshold)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighting parameter\n",
    "\n",
    "$$\n",
    "    p = \\alpha * \\text{titles} + (1-\\alpha) * \\text{abstracts}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52632236"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
